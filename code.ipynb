import json
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import math

with open('Vocab.json', 'r', encoding='utf-8') as f:
    valyrian_dict = json.load(f)

#contains input (Valyrian) and output (Englixsh) pairs
with open('valyrian_sentences.json', 'r', encoding='utf-8') as f:
    corpus_data = json.load(f)

source = [entry['input'] for entry in corpus_data]   # Valyrian sentences
target = [entry['output'] for entry in corpus_data]  # English sentences

target_sentences = ['<start> ' + t + ' <end>' for t in target]

# Tokenize the source sentences
source_tokenizer = Tokenizer()
source_tokenizer.fit_on_texts(source)
source_sequences = source_tokenizer.texts_to_sequences(source)
source_sequences = pad_sequences(source_sequences, padding='post')

# Tokenize the target sentences
target_tokenizer = Tokenizer()
target_tokenizer.fit_on_texts(target_sentences)
print(target_tokenizer.word_index)
target_sequences = target_tokenizer.texts_to_sequences(target_sentences)
target_sequences = pad_sequences(target_sequences, padding='post')


X_train, X_test, y_train, y_test = train_test_split(source_sequences, target_sequences, test_size=0.2, random_state=42)



class Encoder(Model):
    def __init__(self, vocab_size, embedding_dim, enc_units):
        super(Encoder, self).__init__()
        self.embedding = Embedding(vocab_size, embedding_dim)
        self.lstm = LSTM(enc_units, return_sequences=True, return_state=True)

    def call(self, x):
        x = self.embedding(x)
        output, state_h, state_c = self.lstm(x)
        return output, state_h, state_c


class Decoder(Model):
    def __init__(self, vocab_size, embedding_dim, dec_units):
        super(Decoder, self).__init__()
        self.embedding = Embedding(vocab_size, embedding_dim)
        self.lstm = LSTM(dec_units, return_sequences=True, return_state=True)
        self.fc = Dense(vocab_size, activation='softmax')

    def call(self, x, enc_output, state_h, state_c):
        x = self.embedding(x)
        dec_output, dec_state_h, dec_state_c = self.lstm(x, initial_state=[state_h, state_c])
        output = self.fc(dec_output)
        return output, dec_state_h, dec_state_c

vocab_size_src = len(source_tokenizer.word_index) + 1
vocab_size_tgt = len(target_tokenizer.word_index) + 1
embedding_dim = 256
units = 512

encoder = Encoder(vocab_size_src, embedding_dim, units)
decoder = Decoder(vocab_size_tgt, embedding_dim, units)

optimizer = tf.keras.optimizers.Adam()
loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')

def loss_function(real, pred):
    mask = tf.math.not_equal(real, 0)
    loss_ = loss_object(real, pred)
    mask = tf.cast(mask, dtype=loss_.dtype)
    loss_ *= mask
    return tf.reduce_mean(loss_)

@tf.function
def train_step(src_seq, tgt_seq):
    loss = 0
    with tf.GradientTape() as tape:
        enc_output, enc_hidden_h, enc_hidden_c = encoder(src_seq)
        dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c
        dec_input = tgt_seq[:, :-1]  # decoder input does not include the last token
        real = tgt_seq[:, 1:]        # target shifted by one
        pred, dec_hidden_h, dec_hidden_c = decoder(dec_input, enc_output, dec_hidden_h, dec_hidden_c)
        loss = loss_function(real, pred)

    variables = encoder.trainable_variables + decoder.trainable_variables
    gradients = tape.gradient(loss, variables)
    optimizer.apply_gradients(zip(gradients, variables))
    return loss

epochs = 5
batch_size = 16

for epoch in range(epochs):
    total_loss = 0
    for batch in range(len(X_train) // batch_size):
        batch_X = X_train[batch * batch_size: (batch + 1) * batch_size]
        batch_y = y_train[batch * batch_size: (batch + 1) * batch_size]
        batch_loss = train_step(batch_X, batch_y)
        total_loss += batch_loss
    print(f'Epoch {epoch + 1}, Loss: {total_loss.numpy()}')

def calculate_bleu(reference, candidate):
    reference = [reference.split()]
    candidate = candidate.split()
    smoothing = SmoothingFunction().method4
    return sentence_bleu(reference, candidate, smoothing_function=smoothing)

def calculate_perplexity(loss):
    return math.exp(loss)

# Inference example
test_sentence = X_test[0:1]
enc_output, enc_hidden_h, enc_hidden_c = encoder(test_sentence)
dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c

# The token for '<start>' in the target vocabulary
start_token_id = target_tokenizer.word_index['start']
end_token_id = target_tokenizer.word_index['end']
dec_input = np.array([[start_token_id]])

predicted_sentence = []
for t in range(y_test.shape[1]):
    pred, dec_hidden_h, dec_hidden_c = decoder(dec_input, enc_output, dec_hidden_h, dec_hidden_c)
    pred_id = np.argmax(pred[0, -1, :])
    if pred_id == end_token_id:
        break
    predicted_sentence.append(pred_id)
    dec_input = np.array([[pred_id]])

predicted_sentence = ' '.join([target_tokenizer.index_word[i] for i in predicted_sentence if i in target_tokenizer.index_word])
reference_sentence = ' '.join([target_tokenizer.index_word[i] for i in y_test[0] if i in target_tokenizer.index_word and i not in ['<start>', '<end>']])

bleu_score = calculate_bleu(reference_sentence, predicted_sentence)
perplexity = calculate_perplexity(total_loss.numpy() / (len(X_train) // batch_size))

print(f'Predicted: {predicted_sentence}')
print(f'Reference: {reference_sentence}')
print(f'BLEU Score: {bleu_score}')
print(f'Perplexity: {perplexity}')


RESULTS:
Epoch 1, Loss: 4974.90478515625
Epoch 2, Loss: 1790.1435546875
Epoch 3, Loss: 1610.028076171875
Epoch 4, Loss: 1357.037109375
Epoch 5, Loss: 1004.2669677734375
Predicted: portion illa bent ogre
Reference: start portion illa bent pride end
BLEU Score: 0.261698736230754
Perplexity: 1.4943730989752746
